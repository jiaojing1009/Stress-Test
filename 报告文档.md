#报告文档:
- ##题目描述：
假设在⼀组微服务的环境下，有两个服务A和B。实在⼀个⾼并发的⽣产者消费者模型 
服务A：是⼀个多线程的python3代码，可以并发访问/testB, 
服务B：⽤flflask做为http server服务，暴露endpoint /testB 
两个服务部属在⾃⼰的开发机器上，要求通过top命令观察， 
能过调整多线程参数和batch_size，尽可能把cpu打满为⽌。观察出qps和ts_diff
1、A通过http协议访问服务B的endpoint: /testB, 
2、testB内部有⼀个⽣产者消费者模型，每当外部请求数达到batch_size(⽐如为5)的时候，服务B才批 量调⽤⼀次AI
![运行模型](https://github.com/jiaojing1009/Stress-Test/blob/master/%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B.png)

- ##代码
- ####A端：
```
#!/usr/bin/python
# # -*- coding: UTF-8 -*-
from concurrent.futures import ThreadPoolExecutor
import requests
import time
import json
from requests import exceptions

executor = ThreadPoolExecutor(max_workers=127)


# 封装结果集
class Result:
    def __init__(self, successRequest, failureRequest, response, totalTs):
        self.successRequest = successRequest  # 成功的请求数
        self.failureRequest = failureRequest  # 失败的请求数
        self.response = response  # 请求结果
        self.totalTs = totalTs  # 总共耗时


result = Result(0, 0, '', 0)  # 初始化结果（成功的请求数，失败的请求数，请求结果）


def run():
    # 通过exceptions异常来判断请求是否成功
    try:
        t = time.time()
        now = int(round(t * 1000))
        payload = {'ts': now, 'url': 'http://www.pyimagesearch.com/wp-content/uploads/2015/01/google_logo.png'}
        response = requests.post("http://127.0.0.1:5000/testB", data=json.dumps(payload), timeout=3)  # 发起网络请求
        result.response = response
        if 'ts_diff' not in response.json() and 'error' not in response.json():
            print("Json数据格式异常")
            result.failureRequest = result.failureRequest + 1
    except exceptions.Timeout as e:
        print('请求超时')
        result.failureRequest = result.failureRequest + 1
        # print('请求超时：' + str(e.message))

    except exceptions.HTTPError as e:
        print('http请求错误')
        result.failureRequest = result.failureRequest + 1
        # print('http请求错误:' + str(e.message))

    except exceptions.URLRequired as e:
        print('URL缺失异常')
        result.failureRequest = result.failureRequest + 1
        # print('URL缺失异常:' + str(e.message))

    else:
        if response.status_code == 200:
            # 如果请求数量不足 batch_size 则输出error错误
            if 'error' in response.json():
                print("请求错误:" + str(response.json()['error']))
                result.failureRequest = result.failureRequest + 1

            else:
                print('请求耗时:' + str(response.json()['ts_diff'])) \

                result.totalTs = result.totalTs + float(response.json()['ts_diff']) # 记录ts_diff总共耗时
                result.successRequest = result.successRequest + 1  # 记录成功的请求次数
                pass
            # 如果请求数量过多，导致B服务器队列过长，则输出busy
            if 'busy' in result.response.json():
                print(str(response.json()['busy']))
        else:
            result.failureRequest = result.failureRequest + 1
            print('请求错误：' + str(response.status_code) + ',' + str(response.reason))
    # 当返回的请求数量达到 2000 个时，计算出QPS
    if result.successRequest + result.failureRequest == 2000:
        endTime = time.time()
        endTime = int(round(endTime * 1000))
        QBS = float(result.successRequest / (endTime - starTime))
        print("===========结果============")
        print("总共执行时间:" + str(endTime - starTime) + "ms")
        print("成功的请求总数为: " + str(result.successRequest))
        print("失败的请求总数为: " + str(result.failureRequest))
        print("平均ts_diff为:" + str(float(result.totalTs / 2000)) + "ms")
        print("QPS为: " + str(QBS * 1000))


def createThreading():
    for i in range(2000):
        print("-----------in-----------")
        # 当请求数量过多时，sleep3秒
        if 'busy' in result.response:
            time.sleep(3)
        t = executor.submit(run)


if __name__ == '__main__':
    starTime = time.time()
    starTime = int(round(starTime * 1000))
    createThreading()
    # run()
```
- ####B端
```#!/usr/bin/python
# -*- coding: UTF-8 -*-

from flask import request, Flask
import threading
import queue
import json
import time
import re
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor


app = Flask(__name__)

q = queue.Queue(1000)  # 建立一个队列长度为1000，较长的队列可以作为缓冲
executor = ThreadPoolExecutor(max_workers=127)  # 线程池
process = ProcessPoolExecutor(10)


# 封装数据
class Item:
    def __init__(self, event, data, feedback):
        self.event = event
        self.data = data
        self.feedback = feedback


# 接收线程请求
@app.route('/testB', methods=['POST'])
def getRequest():
    event = threading.Event()  # 创建一个事件管理标志
    if 'ts' not in json.loads(request.get_data()) or 'url' not in json.loads(request.get_data()):
        print("请求中的数据有误")
    item = Item(event, request.get_data(), "NULL")  # 初始化线程请求
    # print("收到了")
    q.put(item)
    # print(q.qsize())
    if q.qsize() >= 800:  # 判断队列长度是否过长
        item.feedback = {
            'busy': '请求数量过于密集'
        }
        return item.feedback
    event.wait()  # 线程阻塞
    return item.feedback


# 处理线程请求
def revQueue(batch_size):
    flag = 0  # flag 用于标记 距离上一次执行AI操作的时间
    nowTime = time.time()
    nowTime = int(round(nowTime * 1000))
    while 1:
        if flag == 0:
            nowTime = time.time()
            nowTime = int(round(nowTime * 1000))

        # 当请求队列长度大于batch_size，进行统一处理
        if q.qsize() >= batch_size:
            rev = queue.Queue(batch_size)
            for i in range(batch_size):
                item = q.get()
                rev.put(item)
            pass
            tt = executor.submit(handleAI, rev, batch_size)  # 进行AI处理
            flag = 0  # 重置flag为0，重新获取当前时间

            # 当接收 不到batch_size数量请求，或者长时间没执行AI处理时
        else:
            flag = 1
            afterTime = time.time()
            afterTime = int(round(afterTime * 1000))  # 获取时间差
            if afterTime - nowTime >= 1000:  # 大于1500毫秒时 返回错误信息
                print("超时处理")
                for i in range(q.qsize()):
                    item = q.get()
                    feedback = {
                        'error': '请求数量未满足需求'
                    }
                    item.feedback = feedback
                    event = item.event
                    event.set()  # 唤醒线程
                flag = 0  # 重置flag为0，重新获取当前时间
                pass


# AI处理
def handleAI(queue, batch_size):
    # handle AI
    print("---------- handle AI Start-----------")
    print("---------------- AI ----------------")
    print("-----------handle AI OK--------------")
    for x in range(batch_size):
        item = queue.get()
        data = str(item.data, 'utf-8')
        data = re.sub('\'', '\"', data)
        data = json.loads(data)
        rects = data['ts']
        rects = int(rects)
        t = time.time()
        now = int(round(t * 1000))
        # 封装返回的数据
        feedback = {
            'rec_ts': rects,
            'resp_ts': now,
            'ts_diff': now - rects
        }
        item.feedback = feedback
        event = item.event
        event.set()  # 唤醒线程


if __name__ == '__main__':
    # app.debug = True
    for i in range(10):
        process.submit(getRequest)

    for i in range(3):
        t = executor.submit(revQueue, 10)

    app.run()
```

